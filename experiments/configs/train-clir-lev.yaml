# pytorch_lightning==2.2.2
seed_everything: 42
trainer:
  accelerator: gpu
  strategy: ddp_find_unused_parameters_true
  devices: 1
  num_nodes: 1
  precision: 32
  logger: true
  callbacks:
  - class_path: pytorch_lightning.callbacks.early_stopping.EarlyStopping
    init_args:
      monitor: eval/loss
      patience: 3
  - class_path: pytorch_lightning.callbacks.ModelCheckpoint
    init_args:
      save_top_k: 1
      monitor: eval/loss
      save_last: true
  - class_path: pytorch_lightning.callbacks.LearningRateMonitor
    init_args:
      logging_interval: step
  fast_dev_run: false
  max_epochs: 10
  min_epochs: 1
  max_steps: -1
  min_steps: null
  max_time: null
  limit_train_batches: null
  limit_val_batches: null
  limit_test_batches: null
  limit_predict_batches: null
  overfit_batches: 0.0
  val_check_interval: 5000
  check_val_every_n_epoch: 1
  num_sanity_val_steps: null
  log_every_n_steps: 100
  enable_checkpointing: null
  enable_progress_bar: true
  enable_model_summary: null
  accumulate_grad_batches: 1
  gradient_clip_val: 2.0
  gradient_clip_algorithm: "norm"
  deterministic: null
  benchmark: null
  inference_mode: true
  use_distributed_sampler: false
  profiler: null
  detect_anomaly: false
  barebones: false
  plugins: null
  sync_batchnorm: false
  reload_dataloaders_every_n_epochs: 1
  default_root_dir: experiments/logs/train-clir
ckpt_path: null
model:
  class_path: clir.train.trainee.BiEncoder
  init_args:
    model_name_or_path: null
    vocab_size: 34852
    pad_token_id: 1
    type_vocab_size: 1
    freeze_regex: null
    gradient_checkpointing: false
    warmup_steps: 5000
    lr_scheduler: isqrt
    sqrt_lr_update_factor: null
    lr: 2.0e-05
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    weight_decay: 0.0
    label_smoothing: 0.1
    label_smoothing_bow: 0.1
    normalize: true
    temp_lr: 1e-2
    bow_lr: 1e-2
    bow_multiplicator: null
    bow_loss: false
    bow_loss_factor: null
    lev_train: true
data:
  class_path: clir.train.data.DataModuleMMap
  init_args:
    dict_path: experiments/data/multidomain/data-bin/dict.en.txt
    dataset_path: experiments/data/multidomain/data-bin
    src_lang: en
    tgt_lang: fr
    train: true
    valid: true
    test: false
    max_positions: 256
    max_tokens: 6000
    max_sentences: 128
